{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Validation: The Bias-Variance Tradeoff and the Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Explain** the bias-variance tradeoff and the correlative notions of underfit and overfit models\n",
    "- **Describe** a train-test split and **explain** its purpose in the context of predictive statistics / machine learning\n",
    "- **Explain** the algorithm of cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this point, we have seen different ways to create models from our data through different linear regression techniques. That's great - but, just like a student practicing for a big end-of-the-year test, we want to make sure our _models_ are ready to predict on data they haven't seen yet. We want to know if the models we make are ready to make predictions for data in the \"wild\". \n",
    "\n",
    "Usually, when our model is ready to be used in the \"real world\" we refer to this as putting our model into **production** or **deploying** our model. The data it will use to make predictions will then be **data it's never seen before**. \n",
    "\n",
    "Our training is the data we have in our notebooks when we build the model - but, in supervised learning, we already have the right answers, just like a student taking a practice test who can still check their own answers in the back of the book. But on the big test, aka when the model is out making predictions in the real world (when it's in production or after it's been deployed), we want to know those predictions are useful. We want to know we'll score just as well on the big test as the practice test!\n",
    "\n",
    "But you might be thinking, how do I make sure my model I've been cultivating with my own data is ready? This is where we ***model validation*** techniques to ensure our model can **generalize** to data it hasn't directly seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go over how to ensure our model is ready, but first we have to review how our model can make errors in the context of the **bias-variance tradeoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## High-Bias vs High-Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can break up how the model makes mistakes (the error) by saying there are three parts:\n",
    "\n",
    "- Error inherent of the data (noise): **irreducible error**\n",
    "- Error from being not capturing patterns (too simple): **bias**\n",
    "- Error from using patterns in the data but don't generalize well (too complex): **variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/optimal_bias_variance.png\" width=600>\n",
    "     \n",
    "[[Image Source]](http://scott.fortmann-roe.com/docs/BiasVariance.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**High-bias** algorithms tend to be less complex, with simple or rigid underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/noisy-sine-linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- They train models that are consistent, but inaccurate on average.\n",
    "    - These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "- The following sorts of difficulties could lead to high bias:\n",
    "  - We did not include the correct predictors\n",
    "  - We did not take interactions into account\n",
    "  - We missed a non-linear (polynomial) relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "      \n",
    "High-bias models are generally **underfit**: The models have not picked up enough of the signal in the data. And so even though they may be consistent, they don't perform particularly well on the initial data, and so they will be consistently inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On the other hand, **high-variance** algorithms tend to be more complex, with flexible underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/noisy-sine-decision-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- They train models that are accurate on average, but inconsistent.\n",
    "    - These include non-linear or non-parametric algorithms such as decision trees and nearest-neighbor models.\n",
    "- The following sorts of difficulties could lead to high variance:\n",
    "  - We included an unreasonably large number of predictors;\n",
    "  - We created new features by squaring and cubing each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "High variance models are **overfit**: The models have picked up on the noise as well as the signal in the data. And so even though they may perform well on the initial data, they will be inconsistently accurate on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Balancing Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While we build our models, we have to keep this relationship in mind.  If we build complex models, we risk overfitting our models.  Their predictions will vary greatly when introduced to new data.  If our models are too simple, the predictions as a whole will be inaccurate.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/noisy-sine-third-order-polynomial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal is to build a model with enough complexity to be accurate, but not too much complexity to be erratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/Bias-vs.-Variance-v5-2-darts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Minimize Bias and Variance\n",
    "\n",
    "### Combat Underfitting (Bias)\n",
    "\n",
    "**Bias**: Error introduced by approximating a real-life problem (which may be extremely complicated) by a much simpler model (because the model is too simple to capture the underlying pattern)\n",
    "\n",
    "**The Solution:** evaluate the performance of our models, using a scoring metric, which will help us catch if a model is underfit - if it's performing quite poorly, it probably isn't capturing the relationship in our data! \n",
    "\n",
    "### Combat Overfitting (Variance)\n",
    "\n",
    "**Variance**: Amount by which our model would change if we estimated it using a different training dataset (because the model is over-learning from the training data)\n",
    "\n",
    "**The Solution:** don't train your model on ALL of your data, but keep some of it in reserve to test on, in order to simulate how it will work on new/incoming data.\n",
    "\n",
    "<img src='images/testtrainsplit.png' width =550 />\n",
    "\n",
    "> - **training set** —a subset to train a model.\n",
    "> - **test set**—a subset to test the trained model.\n",
    "\n",
    "How does this fight against overfitting? By witholding data from the training process, we are testing whether the model actually _generalizes_ well. If it does poorly on the test set, it's a good sign that our model learned too much noise from the train set and is overfit! \n",
    "\n",
    "![arrested development gif, found by Andy](https://heavy.com/wp-content/uploads/2013/05/tumblr_mjm9fqhrle1rvnnvyo6_250.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# run this for Google Colab compatability\n",
    "#!wget https://raw.githubusercontent.com/mark-barbour/ds_flex_lectures/refs/heads/main/phase_2/data/Credit.csv -P data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit data from https://www.kaggle.com/avikpaul4u/credit-card-balance\n",
    "\n",
    "Target: `Balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data - only using these cols\n",
    "df = pd.read_csv('data/Credit.csv', \n",
    "                 usecols=['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train_test_split function from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to define our X and y\n",
    "# Aka - what are we predicting? what are we using to predict?\n",
    "\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our X\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our y\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split here!\n",
    "# Set test_size = .33\n",
    "# Set random_state = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore our X_train shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore our X_test shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did it split correctly?\n",
    "len(X_train + X_test) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does X_train look like?\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **YOU SHOULD ALWAYS START WITH A TRAIN TEST SPLIT.**\n",
    "\n",
    "**FOR THE PROJECT, YOU WILL BE _REQUIRED_ TO WORK WITH A TRAIN TEST SPLIT**\n",
    "\n",
    "Note - for the checkpoints and code challenge, follow the instructions given - they might not require a train/test split as they attempt to keep things simple.\n",
    "\n",
    "BUT we're going to use it in this session! Let's see what this looks like in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a full train_df to evaluate our options\n",
    "# This isn't totally necessary, just will let us visualize later\n",
    "# You can use this if you prefer the ols method instead of OLS or sklearn\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the training data to make any modeling decisions!\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code to evaluate our options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What one variable should we use to start?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the OLS method\n",
    "# Set up and fit the model\n",
    "simple_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check your results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate!\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "We have a trained model... what can we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our predictions!\n",
    "simple_train_preds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just looking at two variables... we can visualize this!\n",
    "\n",
    "# Plot our points, rating vs balance, as a scatterplot\n",
    "plt.scatter(train_df['Rating'], train_df['Balance'])\n",
    "\n",
    "# Plot the line of best fit!\n",
    "plt.plot(train_df['Rating'], simple_train_preds, color='black')\n",
    "\n",
    "plt.ylabel('Credit Card Balance')\n",
    "plt.xlabel('Credit Rating')\n",
    "plt.title('Relationship between Credit Rating and Credit Card Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to our actual train values...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can score our models without relying on the statsmodels output!\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use sklearn to score our model, too:\n",
    "\n",
    "# Score our training data - the same as from our summary!\n",
    "# This function requires two inputs: y_true and y_pred:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can now predict for our test set!\n",
    "simple_test_preds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score our testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate:\n",
    "\n",
    "- R2 Score on test set is worse than R2 score on train set - may be overfit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One last thing - can visualize both train and test set!\n",
    "\n",
    "# Plot our training data\n",
    "plt.scatter(train_df['Rating'], train_df['Balance'], color='blue', label='Training')\n",
    "# Plot our testing data\n",
    "plt.scatter(test_df['Rating'], test_df['Balance'], color='green', label='Testing')\n",
    "\n",
    "\n",
    "# Plot the line of best fit\n",
    "plt.plot(train_df['Rating'], simple_train_preds, color='black')\n",
    "# Plotting for the test data just to show it's the same!\n",
    "plt.plot(test_df['Rating'], simple_test_preds, color='red')\n",
    "\n",
    "plt.ylabel('Credit Card Balance')\n",
    "plt.xlabel('Credit Rating')\n",
    "plt.title('Relationship between Credit Rating and Credit Card Balance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Same as simple linear regression, but with more inputs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our X_train info...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to prep our data... can do that all with sklearn!\n",
    "# Time to scale!\n",
    "# Import our scaler\n",
    "\n",
    "# Instantiate our scaler\n",
    "\n",
    "# Fit our scaler \n",
    "\n",
    "# Transform training data\n",
    "\n",
    "# Transform testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit` vs `transform` ?\n",
    "\n",
    "`fit` allows the scaler to learn the patterns _only from the training data_! \n",
    "\n",
    "Why does that matter?\n",
    "\n",
    "- mean and standard deviation are affected by the data in the rows - so, if we fit to the whole dataset instead of just the training data, the scaler would learn patterns influenced by the test data!\n",
    "\n",
    "`transform` allows the scaler to apply the pattern it learns - can do so on both the train and test sets, so long as they're equivalent (aka have the same columns and were prepared the same way!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Should You Ever Fit on Your Test Set?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![no](https://media.giphy.com/media/d10dMmzqCYqQ0/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### **NEVER FIT ON TEST DATA** \n",
    "\n",
    "If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up and fit your model\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check your results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation time!\n",
    "\n",
    "How'd we do? \n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond the $R^2$ Score\n",
    "\n",
    "There are other metrics! \n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\text{MAE}(y, y_\\text{pred}) = \\frac{1}{n} \\sum_{i=0}^{n} \\left| y_i - y_\\text{pred}i \\right|$$\n",
    "\n",
    "- Measures the average magnitude of errors regardless of direction, by calculating the total absolute value of errors and dividing by the number of samples (number of predictions made)\n",
    "- **This error term is in the same units as the target!**\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE}(y, y_\\text{pred}) = \\frac{1}{n} \\sum_{i=0}^{n} (y_i - y_\\text{pred}i)^2$$\n",
    "\n",
    "- Measures the average squared error, by calculating the sum of squared errors for all predictions then dividing by the number of samples (number of predictions)\n",
    "- In other words - this is the Residual Sum of Squares (RSS) divided by the number of predictions!\n",
    "- This error term is **NOT** in the same units as the target!\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\text{RMSE}(y, y_\\text{pred}) = \\sqrt{\\frac{1}{n} \\sum_{i=0}^{n} (y_i - y_\\text{pred}i)^2}$$\n",
    "\n",
    "- Measures the square root of the average squared error, by calculating the sum of squared errors for all predictions then dividing by the number of samples (number of predictions), then taking the square root of all that\n",
    "- **This error term is in the same units as the target!**\n",
    "\n",
    "Note - before, we were _maximizing_ R2 (best fit = largest R2 score). But we'd want to minimize these other error metrics.\n",
    "\n",
    "Documentation: \n",
    "- [Regression Metrics in sklearn](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)\n",
    "- [User Guide for Regression Metrics in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab training set predictions\n",
    "train_preds = None\n",
    "\n",
    "# And test preds\n",
    "test_preds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Metrics:\")\n",
    "# R2\n",
    "print(f\"R2: {r2_score(y_train, train_preds):.3f}\")\n",
    "# MAE\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, train_preds):.3f}\")\n",
    "# MSE\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_train, train_preds):.3f}\")\n",
    "# RMSE - just MSE but set squared=False\n",
    "print(f\"Root Mean Squared Error: {mean_squared_error(y_train, train_preds, squared=False):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Metrics:\")\n",
    "# R2\n",
    "print(f\"R2: {r2_score(y_test, test_preds):.3f}\")\n",
    "# MAE\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, test_preds):.3f}\")\n",
    "# MSE\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, test_preds):.3f}\")\n",
    "# RMSE - just MSE but set squared=False\n",
    "print(f\"Root Mean Squared Error: {mean_squared_error(y_test, test_preds, squared=False):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I said that MAE and RMSE are both in the same units as our target, but you'll see that they are different here. What's the difference?\n",
    "\n",
    "> \"Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.\"\n",
    "\n",
    "-- Source: [\"MAE and RMSE — Which Metric is Better?\"](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret these?\n",
    "\n",
    "- R2: \"Our model accounts for __% of the variance in our target\"\n",
    "- MAE/RMSE: \"Our model's predictions are, on average, about __ off from our actual target values\" (here, balance is likely in dollars - so $___ off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# k-Fold Cross-Validation: Even More Rigorous Validation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our goal of using a test set is to simulate what happens when our model attempts predictions on data it's never seen before. But there's always a chance our model will by chance perform well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is where we could use a more rigorous validation method and so we turn to **k-fold cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![kfolds](images/k_folds.png)\n",
    "\n",
    "[image via sklearn](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this process, we split the dataset into a train set and holdout test sets like usual by performing a shuffling train-test split on the train set.  \n",
    "\n",
    "We then do $k$-number of _folds_ of the training data. This means we divide the training set into different sections or folds. We then take turns on using each fold as a **validation set** (or **dev set**) and train on the larger fraction. Then we calculate a validation score from the validation set the model has never seen. We repeat this process until each fold has served as a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This process allows us to try out training our model and check to see if it is likely to overfit or underfit without touching the holdout test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we think the model is looking good according to our cross-validation using the training data, we retrain the model using all of the training data. Then we can do one final evaluation using the test data. \n",
    "\n",
    "It's important that we hold onto our test data until the end and refrain from making adjustments to the model based on the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have time, we'll put this into practice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note to keep in mind from now on:\n",
    "\n",
    "![\"all models are wrong but some are useful\" quote picture](images/allmodelsarewrong.jpg)\n",
    "\n",
    "[Image Source](https://twitter.com/cwodtke/status/1244433603666178049)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Level Up Exercise: Name that Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the following scenarios and describe them according to bias and variance. There are four possibilities:\n",
    "\n",
    "- a. The model has low bias and high variance.\n",
    "- b. The model has high bias and low variance.\n",
    "- c. The model has both low bias and low variance.\n",
    "- d. The model has both high bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scenario 1**: The model has a low RMSE on training and a low RMSE on test.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scenario 2**: The model has a high $R^2$ on the training set, but a low $R^2$ on the test.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scenario 3**: The model performs well on data it is fit on and well on data it has not seen.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scenario 4**: The model has a low $R^2$ on training but high on the test set.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    d. The model has both high bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scenario 5**: The model leaves out many of the meaningful predictors, but is consistent across samples.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    b. The model has high bias and low variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scenario 6**: The model is highly sensitive to random noise in the training set.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
